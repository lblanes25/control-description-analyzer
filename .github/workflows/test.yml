name: Control Description Analyzer Test Suite

on:
  push:
    branches: [ main, develop, when_split_test ]
  pull_request:
    branches: [ main, develop ]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.8, 3.9, 3.10, 3.11]

    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y python3-dev
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-test.txt
    
    - name: Download spaCy language model
      run: |
        python -m spacy download en_core_web_sm
        python -m spacy download en_core_web_md || echo "en_core_web_md not available, using en_core_web_sm"
    
    - name: Lint with flake8
      run: |
        pip install flake8
        # Stop the build if there are Python syntax errors or undefined names
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        # Exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide
        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
      continue-on-error: true
    
    - name: Run unit tests
      run: |
        pytest tests/unit/ -v --tb=short
    
    - name: Run integration tests
      run: |
        pytest tests/integration/ -v --tb=short
      continue-on-error: true  # Integration tests may fail if external dependencies are missing
    
    - name: Run performance tests
      run: |
        pytest tests/unit/ -v --tb=short -m performance
      continue-on-error: true  # Performance tests may be environment-dependent
    
    - name: Generate coverage report
      run: |
        pytest --cov=src --cov-report=xml --cov-report=html --cov-report=term-missing tests/unit/
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false
    
    - name: Archive coverage HTML report
      uses: actions/upload-artifact@v3
      with:
        name: coverage-report-${{ matrix.python-version }}
        path: htmlcov/
    
    - name: Test report
      uses: dorny/test-reporter@v1
      if: success() || failure()
      with:
        name: Tests Results (${{ matrix.python-version }})
        path: pytest-results.xml
        reporter: java-junit
      continue-on-error: true

  quality-gates:
    runs-on: ubuntu-latest
    needs: test
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python 3.9
      uses: actions/setup-python@v4
      with:
        python-version: 3.9
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-test.txt
        python -m spacy download en_core_web_sm
    
    - name: Run quality gate checks
      run: |
        # Coverage check
        pytest --cov=src --cov-fail-under=85 tests/unit/
        
        # Security check
        pip install safety bandit
        safety check --json || echo "Safety check completed with warnings"
        bandit -r src/ -f json || echo "Bandit security check completed"
        
        # Code complexity check
        pip install radon
        radon cc src/ --min=B || echo "Code complexity check completed"
        
        # Type checking (if type hints are present)
        pip install mypy || echo "MyPy not required for this project"

  documentation:
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python 3.9
      uses: actions/setup-python@v4
      with:
        python-version: 3.9
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install sphinx sphinx-rtd-theme
    
    - name: Generate documentation
      run: |
        # Generate API documentation if sphinx is configured
        echo "Documentation generation placeholder"
        # sphinx-build -b html docs/ docs/_build/html/
    
    - name: Deploy documentation
      # Only deploy docs from main branch
      if: github.ref == 'refs/heads/main'
      run: |
        echo "Documentation deployment placeholder"
        # Could deploy to GitHub Pages or other hosting

  benchmark:
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python 3.9
      uses: actions/setup-python@v4
      with:
        python-version: 3.9
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-test.txt
        python -m spacy download en_core_web_sm
    
    - name: Run benchmark tests
      run: |
        pytest tests/unit/ -v --benchmark-only --benchmark-json=benchmark_results.json
      continue-on-error: true
    
    - name: Store benchmark results
      uses: benchmark-action/github-action-benchmark@v1
      with:
        tool: 'pytest'
        output-file-path: benchmark_results.json
        github-token: ${{ secrets.GITHUB_TOKEN }}
        auto-push: true
      continue-on-error: true

  notify-status:
    runs-on: ubuntu-latest
    needs: [test, quality-gates]
    if: always()
    
    steps:
    - name: Notify on success
      if: needs.test.result == 'success' && needs.quality-gates.result == 'success'
      run: |
        echo "✅ All tests passed and quality gates met!"
        echo "Test Results: ${{ needs.test.result }}"
        echo "Quality Gates: ${{ needs.quality-gates.result }}"
    
    - name: Notify on failure
      if: needs.test.result == 'failure' || needs.quality-gates.result == 'failure'
      run: |
        echo "❌ Tests failed or quality gates not met"
        echo "Test Results: ${{ needs.test.result }}"
        echo "Quality Gates: ${{ needs.quality-gates.result }}"
        exit 1