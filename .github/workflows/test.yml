name: Control Analyzer Test Suite

on:
  push:
    branches: [ main, develop, when_split_test ]
  pull_request:
    branches: [ main, develop ]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        python-version: [3.8, 3.9, "3.10", "3.11", "3.12"]

    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ matrix.python-version }}-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-${{ matrix.python-version }}-
          ${{ runner.os }}-pip-
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential python3-dev
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-test.txt
    
    - name: Download spaCy language model
      run: |
        python -m spacy download en_core_web_sm
        python -m spacy download en_core_web_md || echo "en_core_web_md not available, using en_core_web_sm"
    
    - name: Lint with flake8
      run: |
        pip install flake8
        # Stop the build if there are Python syntax errors or undefined names
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics || true
        # Exit-zero treats all errors as warnings
        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics || true
    
    - name: Run Priority 1 (Critical) Tests
      run: |
        pytest tests/unit/test_core_analyzer.py tests/unit/test_control_classifier.py -v --tb=short
    
    - name: Run Priority 2 (Business Logic) Tests
      run: |
        pytest tests/unit/test_business_logic.py -v --tb=short
    
    - name: Run Priority 3 (Integration) Tests
      run: |
        pytest tests/integration/test_integration_points.py -v --tb=short
    
    - name: Run Performance Tests (Non-slow)
      run: |
        pytest tests/performance/ -v --tb=short -m "not slow"
    
    - name: Generate coverage report
      run: |
        pytest tests/unit/ tests/integration/ --cov=src --cov-report=xml --cov-report=html --cov-report=term-missing --cov-fail-under=85
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false
    
    - name: Archive test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: test-results-${{ matrix.python-version }}
        path: |
          coverage.xml
          htmlcov/
          .coverage
    
    - name: Archive performance results
      uses: actions/upload-artifact@v3
      if: success()
      with:
        name: performance-results-${{ matrix.python-version }}
        path: |
          .benchmarks/
  
  slow-tests:
    runs-on: ubuntu-latest
    needs: test
    if: github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/develop')
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: "3.11"
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-test.txt
        python -m spacy download en_core_web_sm
    
    - name: Run Slow Performance Tests
      run: |
        pytest tests/performance/ -v --tb=short -m "slow" --timeout=300
    
    - name: Archive slow test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: slow-test-results
        path: |
          .benchmarks/

  quality-gates:
    runs-on: ubuntu-latest
    needs: test
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: "3.11"
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-test.txt
        python -m spacy download en_core_web_sm
    
    - name: Run quality gate checks
      run: |
        # Coverage check - ensuring 85% minimum
        pytest tests/unit/ tests/integration/ --cov=src --cov-fail-under=85
        
        # Security check
        pip install safety bandit
        safety check --json --output safety-report.json || echo "Safety check completed with warnings"
        bandit -r src/ -f json -o bandit-report.json || echo "Bandit security check completed"
        
        # Code quality check
        pip install pylint
        pylint src/ --fail-under=7.0 || echo "Pylint check completed"
        
        # Type checking
        pip install mypy
        mypy src/ --ignore-missing-imports || echo "MyPy check completed"
    
    - name: Archive quality results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: quality-check-results
        path: |
          safety-report.json
          bandit-report.json

  benchmark:
    runs-on: ubuntu-latest
    needs: test
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: "3.11"
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-test.txt
        python -m spacy download en_core_web_sm
    
    - name: Run benchmark tests
      run: |
        pytest tests/performance/ --benchmark-only --benchmark-json=benchmark_results.json || echo "Benchmark tests completed"
    
    - name: Archive benchmark results
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results
        path: benchmark_results.json

  notify-status:
    runs-on: ubuntu-latest
    needs: [test, quality-gates]
    if: always()
    
    steps:
    - name: Test Summary
      run: |
        echo "## Test Results Summary" >> $GITHUB_STEP_SUMMARY
        echo "- **Test Suite**: ${{ needs.test.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Quality Gates**: ${{ needs.quality-gates.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Branch**: ${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Commit**: ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
    
    - name: Notify on success
      if: needs.test.result == 'success' && needs.quality-gates.result == 'success'
      run: |
        echo "✅ All tests passed and quality gates met!"
    
    - name: Notify on failure
      if: needs.test.result == 'failure' || needs.quality-gates.result == 'failure'
      run: |
        echo "❌ Tests failed or quality gates not met"
        exit 1