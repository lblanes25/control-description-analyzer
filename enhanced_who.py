import re


def identify_control_action_subjects(doc):
    """Identify the subjects of control action verbs in the text"""
    control_verbs = [
        "review", "approve", "verify", "check", "validate", "reconcile",
        "examine", "analyze", "evaluate", "assess", "monitor", "track",
        "investigate", "inspect", "audit", "oversee", "supervise", "ensure",
        "perform", "execute", "conduct", "disable", "enforce", "generate",
        "address", "compare", "maintain", "identify", "correct", "update",

        # New preventive action verbs
        "prevent", "block", "restrict", "limit", "deny", "prohibit",
        "filter", "reject", "intercept", "stop", "lock", "validate",
        "authenticate", "authorize", "encrypt", "secure", "protect",
        "constrain", "segregate", "isolate", "separate", "control",
        "alert", "detect", "flag", "route", "assign", "allocate",
        "configure", "enforce", "apply", "implement", "check"
    ]

    subjects = []

    for token in doc:
        if token.lemma_.lower() in control_verbs:
            for child in token.children:
                if child.dep_ in ("nsubj", "nsubjpass"):
                    for chunk in doc.noun_chunks:
                        if child.i >= chunk.start and child.i < chunk.end:
                            subjects.append({
                                "text": chunk.text,
                                "verb": token.text,
                                "verb_lemma": token.lemma_,
                                "is_passive": child.dep_ == "nsubjpass",
                                "start": chunk.start,
                                "end": chunk.end
                            })
                            break

    return subjects


def identify_performer_roles(subjects, text):
    """Categorize performers as primary, secondary, or escalation"""
    roles = {
        "primary": [],
        "secondary": [],
        "escalation": []
    }

    # Look for escalation patterns
    escalation_patterns = [
        r'escalated to (?:the\s+)?([a-zA-Z\s]+(?:manager|director|supervisor|analyst|officer))',
        r'reported to (?:the\s+)?([a-zA-Z\s]+(?:manager|director|supervisor|analyst|officer))',
        r'elevated to (?:the\s+)?([a-zA-Z\s]+(?:manager|director|supervisor|analyst|officer))',
        r'notified to (?:the\s+)?([a-zA-Z\s]+(?:manager|director|supervisor|analyst|officer))',
        r'alert(?:s|ed)? (?:the\s+)?([a-zA-Z\s]+(?:manager|director|supervisor|analyst|officer))'
    ]

    escalation_performers = []
    for pattern in escalation_patterns:
        for match in re.finditer(pattern, text, re.IGNORECASE):
            escalation_performers.append(match.group(1).strip())

    # Look for preparation patterns
    preparation_patterns = [
        r'prepared by (?:the\s+)?([a-zA-Z\s]+)',
        r'completed by (?:the\s+)?([a-zA-Z\s]+)',
        r'generated by (?:the\s+)?([a-zA-Z\s]+)',
        r'performed by (?:the\s+)?([a-zA-Z\s]+)',
        r'created by (?:the\s+)?([a-zA-Z\s]+)'
    ]

    preparation_performers = []
    for pattern in preparation_patterns:
        for match in re.finditer(pattern, text, re.IGNORECASE):
            preparation_performers.append(match.group(1).strip())

    # Categorize subjects
    for subject in subjects:
        subject_text = subject["text"].strip().lower()

        # Check if this is an escalation performer
        if any(performer.lower() in subject_text for performer in escalation_performers):
            roles["escalation"].append(subject)
        # Check if this is a preparation performer
        elif any(performer.lower() in subject_text for performer in preparation_performers):
            roles["secondary"].append(subject)
        # If associated with strong control verbs, likely primary
        elif subject["verb_lemma"].lower() in ["review", "approve", "verify", "validate", "reconcile", "examine",
                                               "analyze"]:
            roles["primary"].append(subject)
        else:
            # Default to secondary if role is unclear
            roles["secondary"].append(subject)

    return roles


def calculate_role_adjusted_confidence(entity, role_type):
    """Apply role-specific adjustments to confidence scores"""
    if role_type == "primary":
        # Boost primary performers
        return min(1.0, entity["score"] * 1.2)
    elif role_type == "secondary":
        # Secondary performers (preparers, generators) should have less impact
        return entity["score"] * 0.5
    elif role_type == "escalation":
        # Escalation performers should have minimal impact on WHO scoring
        return entity["score"] * 0.3
    else:
        return entity["score"]


def detect_control_structure(text):
    """Detect common control description structures and identify primary performers"""

    # Pattern: "[Performer] [verb] [object] [prepared by/from] [secondary performer]"
    primary_performer_patterns = [
        r'(?:the\s+)?([a-zA-Z\s]+(?:manager|director|supervisor|analyst|officer|administrator|controller|accountant|auditor|team))\s+(reviews|approves|verifies|reconciles|validates|examines|analyzes|monitors)',
        r'(?:the\s+)?([a-zA-Z\s]+(?:manager|director|supervisor|analyst|officer|administrator|controller|accountant|auditor|team))\s+is\s+responsible\s+for',
        r'(?:the\s+)?([a-zA-Z\s]+(?:manager|director|supervisor|analyst|officer|administrator|controller|accountant|auditor|team))\s+ensures'
    ]

    primary_performers = []
    for pattern in primary_performer_patterns:
        for match in re.finditer(pattern, text, re.IGNORECASE):
            primary_performers.append({
                "text": match.group(1).strip(),
                "verb": match.group(2) if len(match.groups()) > 1 else "responsible",
                "position": match.start()
            })

    return primary_performers


def classify_entity_type(text, nlp):
    """Classify an entity as human, system, or non-performer"""
    text_lower = text.lower()

    human_indicators = [
        # Enhanced for finance roles
        "manager", "director", "supervisor", "analyst", "specialist", "officer",
        "coordinator", "lead", "team", "staff", "personnel", "employee",
        "individual", "person", "accountant", "controller", "auditor", "administrator",
        "executive", "chief", "head", "president", "ceo", "cfo", "cio", "vp",
        "vice president", "secretary", "treasurer", "owner", "preparer", "reviewer",
        # Added more specificity for finance roles
        "finance", "accounting", "accounts receivable", "accounts payable", "treasury",
        "financial reporting", "tax", "payroll", "billing", "credit", "collections",
        "financial controller", "financial analyst", "budget analyst", "compliance"
    ]

    system_indicators = [
        "system", "application", "software", "platform", "database", "server",
        "program", "script", "job", "batch", "workflow", "algorithm", "automated",
        "automatic", "tool", "module", "interface", "api", "service", "function",
        "process", "scheduled", "timer", "daemon", "bot", "routine", "task"
    ]

    non_performer_indicators = [
        "limit", "threshold", "policy", "procedure", "standard", "regulation",
        "account", "transaction", "balance", "report", "document", "record",
        "exception", "error", "issue", "finding", "discrepancy", "review",
        "control", "entry", "activity", "access", "instance"
    ]

    # Check if the entity contains human role indicators
    if any(indicator in text_lower for indicator in human_indicators):
        return "human"

    # Check if the entity contains system indicators
    if any(indicator in text_lower for indicator in system_indicators):
        return "system"

    # Check if the entity contains non-performer indicators
    if any(indicator in text_lower for indicator in non_performer_indicators):
        return "non-performer"

    # Use NLP to check for person or organization entities
    doc = nlp(text)

    for ent in doc.ents:
        if ent.label_ in ("PERSON", "ORG"):
            return "human"

    # Default to unknown if no clear classification
    return "unknown"


def is_valid_performer(text):
    """Check if a text segment is likely to be a valid performer"""
    # Filter out common false positives
    false_positives = ["changes", "results", "distributions", "communications"]

    if any(fp.lower() == text.lower() for fp in false_positives):
        return False

    # Should have a role or organizational indicator
    role_indicators = ["manager", "director", "team", "staff", "officer", "group", "department"]
    has_role = any(indicator in text.lower() for indicator in role_indicators)

    # Should not be just a verb or action
    action_only = re.match(r'^\w+ed$', text.strip()) is not None

    return has_role and not action_only


def is_valid_performer_in_context(entity, full_text, nlp):
    """Check if an entity is likely to be a control performer based on surrounding context"""
    # Look for verb patterns around the entity
    window_size = 15  # words

    # Find the position of the entity in the text
    entity_pos = full_text.find(entity)
    if entity_pos == -1:
        return False

    # Get text window around entity
    start_pos = max(0, entity_pos - 100)
    end_pos = min(len(full_text), entity_pos + len(entity) + 100)
    context_window = full_text[start_pos:end_pos]

    # Check for control verb patterns
    control_patterns = [
        r'(review|approve|verify|monitor|check|validate|reconcile)\s+by',
        r'(reviewed|approved|verified|monitored|checked|validated|reconciled)\s+by',
        r'performed\s+by',
        r'conducted\s+by',
        r'executed\s+by',
        r'is\s+responsible\s+for'
    ]

    for pattern in control_patterns:
        if re.search(pattern, context_window, re.IGNORECASE):
            return True

    # Check if the entity appears to be the subject of a control verb
    doc = nlp(context_window)
    for token in doc:
        if token.dep_ == "nsubj" and token.head.lemma_ in ["review", "approve", "verify", "monitor"]:
            for chunk in doc.noun_chunks:
                if token.i >= chunk.start and token.i < chunk.end:
                    chunk_text = chunk.text.lower()
                    if entity.lower() in chunk_text:
                        return True

    return False

def calculate_who_confidence(entity, control_type=None, frequency=None):
    """Calculate confidence score for a WHO entity with improved scoring"""
    # Increased base score
    base_score = 0.7  # Start with higher base score

    # Adjust for entity type
    if entity["type"] == "human":
        base_score += 0.2
        # Extra boost for specific managerial roles
        if any(term in entity["text"].lower() for term in ["manager", "director", "supervisor", "officer"]):
            base_score += 0.2  # Extra boost for specific roles
        # Even higher boost for very specific finance roles
        if any(term in entity["text"].lower() for term in [
            "accounts receivable manager", "accounts payable manager",
            "finance manager", "financial controller", "accounting manager"
        ]):
            base_score += 0.3  # Additional boost for specific finance roles
    elif entity["type"] == "system":
        base_score += 0.1
    elif entity["type"] == "non-performer":
        base_score = 0.1  # Very low confidence for non-performers

    # Reduced penalty for passive voice
    if entity.get("is_passive", False):
        base_score -= 0.1  # Reduced penalty for passive voice

    # Adjust for control type consistency
    if control_type:
        control_type_lower = control_type.lower()

        if "automated" in control_type_lower and entity["type"] == "system":
            base_score += 0.2
        elif "automated" in control_type_lower and entity["type"] == "human":
            base_score -= 0.2
        elif "manual" in control_type_lower and entity["type"] == "human":
            base_score += 0.2
        elif "manual" in control_type_lower and entity["type"] == "system":
            base_score -= 0.2

    # Adjust for frequency consistency
    if frequency:
        frequency_lower = frequency.lower()

        if any(term in frequency_lower for term in ["daily", "continuous"]):
            if entity["type"] == "system":
                base_score += 0.1
            elif "director" in entity["text"].lower() or "executive" in entity["text"].lower():
                base_score -= 0.1

    # Ensure score is within valid range
    return max(0.1, min(1.0, base_score))


def detect_performer_coreferences(candidates, text):
    """Detect when different terms refer to the same performer and boost the score"""
    if len(candidates) < 2:
        return candidates

    # CHANGE 1: Add special case for "The manager validates" pattern
    if len(candidates) > 0:
        # Look for "the manager validates" pattern which strongly indicates a primary performer
        manager_pattern = re.search(r'the\s+manager\s+(validates|ensures|confirms|reviews)', text, re.IGNORECASE)
        if manager_pattern:
            # Find primary candidate with "manager" in title
            for candidate in candidates:
                if "manager" in candidate["text"].lower():
                    candidate["role"] = "primary"  # Ensure it's marked as primary
                    candidate["score"] = min(1.0, candidate["score"] * 1.5)  # Strong boost
                    break

    # Check for common patterns like "Manager" followed by "The manager"
    for i, candidate in enumerate(candidates):
        if i == 0:
            continue

        # Get the last word of the first performer (usually the title)
        primary_performer = candidates[0]["text"].lower()
        primary_title = primary_performer.split()[-1]
        current_text = candidate["text"].lower()

        # Check if this is a reference like "the manager" to the first performer
        if current_text == "the " + primary_title or current_text == primary_title:
            # This is likely a reference to the first performer
            candidates[0]["score"] = min(1.0, candidates[0]["score"] + 0.3)  # Increased boost
            # Mark this candidate as a coreference
            candidate["is_coreference"] = True
            candidate["refers_to"] = primary_performer

    # Also check for explicit references in the text
    for candidate in candidates:
        if "manager" in candidate["text"].lower():
            match = re.search(r'the\s+manager\s+validates|the\s+manager\s+confirms|the\s+manager\s+ensures',
                              text, re.IGNORECASE)
            if match:
                candidate["score"] = min(1.0, candidate["score"] + 0.2)

    return candidates


def enhanced_who_detection_v2(text, nlp, control_type=None, frequency=None, existing_keywords=None):
    """Enhanced WHO detection with improved context handling and process awareness"""
    if not text or text.strip() == '':
        return {
            "primary": None,
            "secondary": [],
            "confidence": 0,
            "message": "No text provided"
        }

    try:
        doc = nlp(text)

        # Define review_approve_candidate at function level
        review_approve_candidate = None

        # CRITICAL PATTERN: "X is reviewed and approved by Y" - where Y is the WHO
        review_approve_pattern = re.search(
            r'(?:are|is|must be)\s+(?:reviewed|approved|validated|checked|reconciled)(?:\s+and\s+(?:reviewed|approved|validated|checked))?\s+by\s+(?:the\s+)?([a-zA-Z0-9\s\(\)\-\.,]+?(?:team|manager|director|supervisor|analyst|administrator|officer|group|department|specialist|stakeholders|compliance|stakeholder)(?:[,.]\s+including\s+[^\.]+)?)',
            text, re.IGNORECASE
        )

        if review_approve_pattern:
            performer = review_approve_pattern.group(1).strip()

            # Check for "including" pattern which often gives more specific performers
            including_match = re.search(r'including\s+([^\.]+)', performer, re.IGNORECASE)
            if including_match:
                # Get the more specific performers after "including"
                specific_performers = including_match.group(1).strip()
                # Create a high priority candidate with the specific performers
                entity_type = classify_entity_type(specific_performers, nlp)
                confidence = 1.0

                review_approve_candidate = {
                    "text": specific_performers,
                    "verb": "reviews/approves",
                    "type": entity_type,
                    "score": confidence,
                    "position": review_approve_pattern.start(1),
                    "role": "primary",
                    "pattern_match": "critical_passive"
                }
            else:
                # Use the standard performer extracted from the pattern
                entity_type = classify_entity_type(performer, nlp)
                confidence = 1.0

                review_approve_candidate = {
                    "text": performer,
                    "verb": "reviews/approves",
                    "type": entity_type,
                    "score": confidence,
                    "position": review_approve_pattern.start(1),
                    "role": "primary",
                    "pattern_match": "critical_passive"
                }
        # Add special handling for CTRL-011 type cases
        detailed_control_pattern = re.search(
            r'(on\s+a\s+[a-z]+\s+basis|[a-z]+ly),\s+(?:the\s+)?([a-zA-Z\s]+(?:manager|director|supervisor))', text,
            re.IGNORECASE)

        high_priority_candidate = None
        if detailed_control_pattern:
            performer = detailed_control_pattern.group(2).strip()
            entity_type = classify_entity_type(performer, nlp)

            # This is a very clear control structure, give it a high score
            confidence = 1.0  # Maximum confidence

            # Create a high-priority candidate
            high_priority_candidate = {
                "text": performer,
                "verb": "reviews",  # Assume most common verb
                "type": entity_type,
                "score": confidence,
                "position": detailed_control_pattern.start(2),
                "role": "primary"
            }

        # First check for clear control structures to identify primary performers
        primary_performers_from_structure = detect_control_structure(text)

        # SPECIAL HANDLING FOR "BY" PATTERN - Direct regex approach
        # Try to detect phrases like "by the accounting manager" directly
        by_pattern_regex = r'by\s+(?:the\s+)?([a-zA-Z\s]+\s+(?:manager|director|supervisor|analyst|team|administrator|controller|accountant|auditor|officer|staff|employee|specialist|group|department))'
        by_matches = re.finditer(by_pattern_regex, text, re.IGNORECASE)

        # Passive voice pattern detection
        passive_pattern_regex = r'(?:is|are|was|were)\s+(?:\w+ed)(?:\s+and\s+\w+ed)?\s+by\s+(?:the\s+)?([a-zA-Z0-9\s\(\)\-\.,]+?(?:team|manager|director|supervisor|analyst|administrator|officer|group|department|specialist))'
        passive_matches = list(re.finditer(passive_pattern_regex, text, re.IGNORECASE))

        # Try to capture organizational units with parentheses
        org_unit_pattern = r'([A-Z][a-zA-Z0-9\s\(\)\-\.,]+?(?:team|group|unit|department|division))'
        org_matches = list(re.finditer(org_unit_pattern, text))

        # Combine all potential entity matches
        who_candidates = []

        # Add review_approve_candidate if it exists
        if review_approve_candidate:
            who_candidates.append(review_approve_candidate)

        # Process passive matches first (highest confidence)
        for match in passive_matches:
            performer = match.group(1).strip()
            # Skip common false positives or if entire sentence is captured
            if not is_valid_performer(performer) or len(performer) > 50:
                continue

            # Check if this is the subject of the sentence instead of the performer
            # Look for key patterns that indicate this is not a performer
            if re.match(r'all|any|each|every|the', performer.lower()):
                continue

            who_candidates.append({
                "text": performer,
                "verb": "reviewed",
                "type": classify_entity_type(performer, nlp),
                "score": 0.9,
                "position": match.start()
            })

        # If no passive matches, process organizational units
        if not who_candidates:
            for match in org_matches:
                performer = match.group(1).strip()
                if is_valid_performer_in_context(performer, text, nlp):
                    who_candidates.append({
                        "text": performer,
                        "verb": "unknown",
                        "type": "human",
                        "score": 0.7,
                        "position": match.start()
                    })

        # If we have a detailed control pattern, add it first (highest priority)
        if detailed_control_pattern:
            who_candidates.append(high_priority_candidate)

        # If we have any "by [performer]" matches, prioritize these
        for match in by_matches:
            performer = match.group(1).strip()
            entity_type = classify_entity_type(performer, nlp)

            # Skip if somehow this is a non-performer
            if entity_type == "non-performer":
                continue

            # "By" pattern matches get a high confidence because they're very explicit
            confidence = calculate_who_confidence(
                {"text": performer, "type": entity_type, "is_passive": True},
                control_type, frequency
            ) * 1.2  # Boost confidence for explicit "by" patterns

            who_candidates.append({
                "text": performer,
                "verb": "reviewed",  # Assume most common passive verb
                "type": entity_type,
                "score": min(0.95, confidence),  # Cap at 0.95 instead of 0.9
                "position": match.start(),
                "role": "secondary"  # "by" performers are typically secondary
            })

        # Get action subjects and passive performers
        action_subjects = identify_control_action_subjects(doc)
        passive_verbs = [token for token in doc if token.dep_ == "auxpass"]
        by_performers = []

        if passive_verbs:
            # Look for "by" prepositions
            for token in doc:
                if token.text.lower() == "by" and token.dep_ == "prep":
                    for child in token.children:
                        if child.dep_ == "pobj":
                            # Get the complete noun phrase
                            for chunk in doc.noun_chunks:
                                if child.i >= chunk.start and child.i < chunk.end:
                                    by_performers.append({
                                        "text": chunk.text,
                                        "verb": passive_verbs[0].head.text if passive_verbs else "unknown",
                                        "verb_lemma": passive_verbs[0].head.lemma_ if passive_verbs else "unknown",
                                        "is_passive": True,
                                        "start": chunk.start,
                                        "end": chunk.end
                                    })
                            # If no matching noun chunk, use the token directly
                            if not by_performers:
                                by_performers.append({
                                    "text": child.text,
                                    "verb": passive_verbs[0].head.text if passive_verbs else "unknown",
                                    "verb_lemma": passive_verbs[0].head.lemma_ if passive_verbs else "unknown",
                                    "is_passive": True,
                                    "start": child.i,
                                    "end": child.i + 1
                                })

        # Add "by [performer]" to action subjects
        action_subjects.extend(by_performers)

        # If still no subjects but we have passive verbs, check if this is a passive without performer
        if not action_subjects and passive_verbs and not who_candidates:
            # Look for subjects that might be mistaken as performers
            subjects = []
            for token in passive_verbs:
                head = token.head  # This is the main verb
                for child in head.children:
                    if child.dep_ == "nsubjpass":
                        # Filter out known non-performers in passive voice
                        if child.text.lower() in ["changes", "distributions", "communications", "result", "results",
                                                  "materials"]:
                            continue  # These are never performers
                        subjects.append(child.text.lower())

            # Common non-performer subjects in passive constructions
            passive_non_performers = [
                "exception", "exceptions", "issue", "issues",
                "error", "errors", "transaction", "transactions",
                "item", "items", "reconciliation", "reconciliations",
                "document", "documents", "report", "reports",
                "activity", "activities", "access", "processes",
                "account", "accounts", "balance", "balances",
                "entry", "entries", "control", "controls",
                "review", "reviews", "check", "checks",
                "finding", "findings", "discrepancy", "discrepancies"
            ]

            # If all identified subjects are in non-performer list, return zero score
            if all(subj in passive_non_performers for subj in subjects):
                return {
                    "primary": {
                        "text": "Unknown Performer",
                        "verb": passive_verbs[0].head.text if passive_verbs else "unknown",
                        "type": "unknown",
                        "score": 0.0  # Changed from non-zero to zero
                    },
                    "secondary": [],
                    "confidence": 0,  # Changed from non-zero to zero
                    "message": "Passive voice with no performer detected"
                }
            else:
                # Default handling for passive voice (also changed to zero)
                return {
                    "primary": {
                        "text": "Unknown Performer",
                        "verb": passive_verbs[0].head.text if passive_verbs else "unknown",
                        "type": "unknown",
                        "score": 0.0  # Changed from 0.2 to 0.0
                    },
                    "secondary": [],
                    "confidence": 0,  # Changed from 0.2 to 0
                    "message": "Passive voice detected - no clear performer specified"
                }

        # If still no subjects but we have passive verbs, fall back to looking for active subjects in noun chunks
        if not action_subjects and not passive_verbs and not who_candidates:
            # Define extended list of non-performer noun chunks to filter out
            non_performer_chunks = [
                "exception", "exceptions", "record", "records",
                "transaction", "transactions", "issue", "issues",
                "process", "processes", "review", "reviews",
                "error", "errors", "task", "tasks",
                "action", "actions", "item", "items",
                "report", "reports", "document", "documents",
                "reconciliation", "reconciliations", "activity", "activities",
                "access", "finding", "findings", "discrepancy", "discrepancies",
                "account", "accounts", "balance", "balances",
                "approval", "approvals", "entry", "entries",
                "control", "controls", "instance", "instances",
                "policy", "policies", "procedure", "procedures"
            ]

            # Only consider noun chunks that could be actual performers
            for chunk in doc.noun_chunks:
                chunk_text = chunk.text.lower()

                # Skip if chunk contains any non-performer term
                if any(term in chunk_text for term in non_performer_chunks):
                    continue

                # Check if any token in the chunk is a non-performer
                chunk_tokens = [token.lemma_.lower() for token in chunk]
                if any(token in non_performer_chunks for token in chunk_tokens):
                    continue

                entity_type = classify_entity_type(chunk.text, nlp)
                if entity_type == "non-performer":
                    continue

                # Apply less penalty for noun chunks without clear verb association
                confidence = calculate_who_confidence(
                    {"text": chunk.text, "type": entity_type},
                    control_type,
                    frequency
                ) * 0.7  # Less penalty than before

                # Only add if the confidence meets minimum threshold
                if confidence >= 0.3:
                    who_candidates.append({
                        "text": chunk.text,
                        "verb": "unknown",
                        "type": entity_type,
                        "score": confidence,
                        "position": chunk.start,
                        "role": "unknown"
                    })

        # If no candidates at all, return unknown with zero score
        if not who_candidates and not action_subjects:
            return {
                "primary": {
                    "text": "Unknown Performer",
                    "verb": "unknown",
                    "type": "unknown",
                    "score": 0.0
                },
                "secondary": [],
                "confidence": 0,
                "message": "No performer detected"
            }

        # Process action subjects to identify candidates
        for subject in action_subjects:
            # Lemmatize subject text for comparison
            subject_text = subject["text"].lower()

            # Non-performer terms list
            non_performer_terms = [
                "exception", "exceptions", "record", "records",
                "transaction", "transactions", "issue", "issues",
                "process", "processes", "review", "reviews",
                "error", "errors", "task", "tasks",
                "action", "actions", "item", "items",
                "report", "reports", "document", "documents",
                "reconciliation", "reconciliations", "activity", "activities",
                "change", "changes", "distribution", "distributions",
                "communication", "communications", "material", "materials",
                "result", "results", "rating", "ratings",
                "balance", "balances", "filing", "filings",
                "disbursement", "disbursements", "campaign", "campaigns"
            ]

            # Skip if subject contains any non-performer term
            if any(term in subject_text for term in non_performer_terms):
                continue

            entity_type = classify_entity_type(subject["text"], nlp)
            if entity_type == "non-performer":
                continue

            confidence = calculate_who_confidence(
                {"text": subject["text"], "type": entity_type, "is_passive": subject.get("is_passive", False)},
                control_type, frequency
            )

            # Add boost for clear manager/director titles
            if "manager" in subject_text or "director" in subject_text:
                confidence *= 1.2  # Boost confidence for managerial roles

            who_candidates.append({
                "text": subject["text"],
                "verb": subject["verb"],
                "type": entity_type,
                "score": confidence,
                "position": subject["start"],
                "role": "unknown"  # Will be determined later
            })

        # Determine performer roles
        if action_subjects:
            performer_roles = identify_performer_roles(action_subjects, text)

            # Apply role-specific adjustments
            for candidate in who_candidates:
                # Skip candidates we already have a role for (like from detailed_control_pattern)
                if "role" in candidate and candidate["role"] == "primary":
                    continue

                # Find matching subject
                matching_subjects = [s for s in action_subjects if s["text"].lower() == candidate["text"].lower()]
                if matching_subjects:
                    subject = matching_subjects[0]
                    # Determine role type
                    role_type = "primary"
                    if subject in performer_roles["escalation"]:
                        role_type = "escalation"
                    elif subject in performer_roles["secondary"]:
                        role_type = "secondary"

                    # Apply role adjustment
                    candidate["score"] = calculate_role_adjusted_confidence(candidate, role_type)
                    candidate["role"] = role_type

        # Apply boosts from control structure detection
        if primary_performers_from_structure:
            for primary in primary_performers_from_structure:
                # Check if this performer is already in candidates
                matching_candidates = [c for c in who_candidates if primary["text"].lower() in c["text"].lower()]

                if matching_candidates:
                    # Boost existing candidate
                    for candidate in matching_candidates:
                        candidate["score"] = min(1.0, candidate["score"] * 1.3)
                        candidate["role"] = "primary"
                else:
                    # Add new candidate
                    entity_type = classify_entity_type(primary["text"], nlp)
                    confidence = calculate_who_confidence(
                        {"text": primary["text"], "type": entity_type, "is_passive": False},
                        control_type, frequency
                    ) * 1.3  # Boost for structure-detected performers

                    who_candidates.append({
                        "text": primary["text"],
                        "verb": primary.get("verb", "unknown"),
                        "type": entity_type,
                        "score": min(1.0, confidence),
                        "position": primary["position"],
                        "role": "primary"
                    })

        # If still no candidates, return unknown with zero score
        if not who_candidates:
            return {
                "primary": {
                    "text": "Unknown Performer",
                    "verb": "unknown",
                    "type": "unknown",
                    "score": 0.0
                },
                "secondary": [],
                "confidence": 0,
                "message": "No performer detected"
            }

        # Priority override: If we found a critical passive match, ensure it's first
        if review_approve_candidate:
            # Check if it's already in the list
            if all(c.get("pattern_match") != "critical_passive" for c in who_candidates):
                # It's not in the list yet, so insert it
                who_candidates.insert(0, review_approve_candidate)

        # Sort candidates by score and then by position in text
        who_candidates.sort(key=lambda x: (-x["score"], x["position"]))

        # Check if we have identified primary performers
        primary_performers = [c for c in who_candidates if c.get("role") == "primary"]

        if primary_performers:
            # If we have identified primary performers, prioritize them
            who_candidates = primary_performers + [c for c in who_candidates if c.get("role") != "primary"]

            # Boost the first primary performer
            who_candidates[0]["score"] = min(1.0, who_candidates[0]["score"] * 1.3)

        # Check if primary performer appears in first third of text and boost score
        if who_candidates and who_candidates[0]["position"] < len(text) * 0.3:  # In first third of text
            who_candidates[0]["score"] = min(1.0, who_candidates[0]["score"] * 1.5)  # Boost by 50%

        # Detect and handle coreferences (e.g., "Manager" followed by "The manager")
        who_candidates = detect_performer_coreferences(who_candidates, text)

        # Filter out coreferences from secondary performers
        secondary_performers = [c for c in who_candidates[1:] if not c.get("is_coreference", False)]

        # Generate message
        message = ""

        # Check for consistency with control type
        if control_type and who_candidates[0]["type"] != "unknown":
            control_type_lower = control_type.strip().lower()
            performer_type = who_candidates[0]["type"]

            if control_type_lower == "automated" and performer_type == "human":
                message = "Warning: Human performer detected for automated control"
            elif control_type_lower == "manual" and performer_type == "system":
                message = "Warning: System performer detected for manual control"
            # Optional: flag semi-automated mismatches differently
            elif control_type_lower == "semi-automated":
                message = ""  # or a soft hint, like "Mixed control: ensure roles align with automation level"

        # Check for vague performers
        if who_candidates[0]["text"].lower() in ["management", "staff", "team"]:
            message = "Warning: Vague performer detected - consider specifying exact role"

        # Identify primary and secondary performers
        primary = who_candidates[0]

        # Return the final result
        return {
            "primary": primary,
            "secondary": secondary_performers,
            "confidence": primary["score"],
            "message": message
        }

    except Exception as e:
        print(f"Error in enhanced WHO detection: {str(e)}")
        return {
            "primary": None,
            "secondary": [],
            "confidence": 0,
            "message": f"Error: {str(e)}"
        }